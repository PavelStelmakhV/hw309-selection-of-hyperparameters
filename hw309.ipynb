{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpF01ufBRTLWENR5ejjsMn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavelStelmakhV/hw309-selection-of-hyperparameters/blob/main/hw309.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "NoQ5hpr-Qb5h",
        "outputId": "e11ab7d2-7b75-41e9-9e63-f99db1e93571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import layers\n",
        "from keras import callbacks\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import TensorBoard\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import tf_keras\n",
        "\n",
        "# from keras import layers\n",
        "# from keras import models\n",
        "# from keras import regularizers\n",
        "# from keras import callbacks\n",
        "# from keras import initializers\n",
        "\n",
        "\n",
        "# from keras.models import load_model\n"
      ],
      "metadata": {
        "id": "PyW3hSZZt72U"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf ./logs/"
      ],
      "metadata": {
        "id": "RZfzUppEZlwp"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "log_dir = 'logs/hparam_tuning/'"
      ],
      "metadata": {
        "id": "r9l75p-fwODt"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "vVHtrihNt0DT"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test = to_categorical(y_test, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "cxLc_T0SyWMS"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([64, 128, 256]))\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.25, 0.5))\n",
        "# HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.25]))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['sgd', 'rmsprop', 'adam', 'nadam']))\n",
        "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([32, 64, 128, 256]))\n",
        "HP_NUM_LAYERS = hp.HParam('num_layers', hp.Discrete([1, 2, 3, 4]))\n",
        "HP_L2 = hp.HParam('l_2', hp.Discrete([1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER, HP_BATCH_SIZE, HP_NUM_LAYERS, HP_L2],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "  )"
      ],
      "metadata": {
        "id": "8go5GirLDVOm"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_relu(model, neurons, drop_out):\n",
        "  w_init_relu = initializers.HeNormal(seed=42)\n",
        "  b_init = initializers.Zeros()\n",
        "\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(layers.Dense(neurons,\n",
        "                        activation='relu',\n",
        "                        kernel_initializer=w_init_relu,\n",
        "                        bias_initializer=b_init,\n",
        "                        kernel_regularizer=regularizers.L2(l2=hparams[HP_L2]),\n",
        "                        # # kernel_regularizer=regularizers.L1L2(l1=l1, l2=l2),\n",
        "                        bias_regularizer=regularizers.L2(l2=hparams[HP_L2]),\n",
        "                        # # bias_regularizer=regularizers.L1L2(l1=l1, l2=l2),\n",
        "                        activity_regularizer=regularizers.L2(l2=hparams[HP_L2])\n",
        "                        # # activity_regularizer=regularizers.L1L2(l1=l1, l2=l2)\n",
        "                         )\n",
        "  )\n",
        "  model.add(layers.Dropout(drop_out))\n",
        "  return model"
      ],
      "metadata": {
        "id": "BYi3VT0hc-W5"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_model(run_dir, hparams):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "    for num_layer in range(hparams[HP_NUM_LAYERS]):\n",
        "      model = layer_relu(model, hparams[HP_NUM_UNITS], hparams[HP_DROPOUT])\n",
        "      # print(num_layer, hparams[HP_NUM_LAYERS])\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=hparams[HP_OPTIMIZER],\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "\n",
        "    callback_list = [\n",
        "        EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=6),\n",
        "        ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True),\n",
        "        TensorBoard(log_dir=run_dir, histogram_freq=1)\n",
        "    ]\n",
        "    # Train the model\n",
        "    model.fit(x=x_train,\n",
        "              y=y_train,\n",
        "              epochs=2,\n",
        "              batch_size=hparams[HP_BATCH_SIZE],\n",
        "              validation_split=0.13,\n",
        "              callbacks=callback_list,\n",
        "              verbose=0\n",
        "              )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")\n",
        "    return test_accuracy\n"
      ],
      "metadata": {
        "id": "NqxYhvxFD8kh"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(run_dir, hparams, step):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    accuracy = train_test_model(run_dir, hparams)\n",
        "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=step)"
      ],
      "metadata": {
        "id": "hePW-s4PEVde"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_num = 10\n",
        "\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
        "  # for dropout_rate in HP_DROPOUT.domain.values:\n",
        "    for optimizer in HP_OPTIMIZER.domain.values:\n",
        "      for batch_size in HP_BATCH_SIZE.domain.values:\n",
        "        for num_layers in HP_NUM_LAYERS.domain.values:\n",
        "          for l_2 in HP_L2.domain.values:\n",
        "            hparams = {\n",
        "                HP_NUM_UNITS: num_units,\n",
        "                HP_DROPOUT: dropout_rate,\n",
        "                HP_OPTIMIZER: optimizer,\n",
        "                HP_BATCH_SIZE: batch_size,\n",
        "                HP_NUM_LAYERS: num_layers,\n",
        "                HP_L2: l_2\n",
        "            }\n",
        "          run_name = \"run-%d\" % session_num\n",
        "          run_name = run_name + f\"[{optimizer}] d[{dropout_rate}] n[{num_units}] bs[{batch_size}] nl[{num_layers}] [{l_2}]\"\n",
        "          print('--- Starting trial: %s' % run_name)\n",
        "          print({h.name: hparams[h] for h in hparams})\n",
        "\n",
        "          run(log_dir + run_name, hparams, session_num)\n",
        "          session_num += 1"
      ],
      "metadata": {
        "id": "pvUvxZBqEXxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94978e2-7ea7-4684-e140-29d9ecd88fdb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting trial: run-10[adam] d[0.25] n[64] bs[32] nl[1] [0.01]\n",
            "{'num_units': 64, 'dropout': 0.25, 'optimizer': 'adam', 'batch_size': 32, 'num_layers': 1, 'l_2': 0.01}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "# print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "ZMEhnbnl3Xax",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "Cw0tPvrV4VdG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/hparam_tuning"
      ],
      "metadata": {
        "id": "jled6KGdUzai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}