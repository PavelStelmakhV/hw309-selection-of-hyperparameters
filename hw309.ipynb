{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3uDdpYD1Srw3fY+DiPhy+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavelStelmakhV/hw309-selection-of-hyperparameters/blob/main/hw309.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "NoQ5hpr-Qb5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d03719-1459-4186-d2c9-664ef6b79a9b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import layers\n",
        "from keras import callbacks\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import TensorBoard\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import tf_keras\n",
        "\n",
        "# from keras import layers\n",
        "# from keras import models\n",
        "# from keras import regularizers\n",
        "# from keras import callbacks\n",
        "# from keras import initializers\n",
        "\n",
        "\n",
        "# from keras.models import load_model\n"
      ],
      "metadata": {
        "id": "PyW3hSZZt72U"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./logs/"
      ],
      "metadata": {
        "id": "RZfzUppEZlwp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "log_dir = 'logs/hparam_tuning/'"
      ],
      "metadata": {
        "id": "r9l75p-fwODt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "vVHtrihNt0DT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test = to_categorical(y_test, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "cxLc_T0SyWMS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([512, 1024]))\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.3, 0.5))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'adamax', 'nadam']))\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "  )"
      ],
      "metadata": {
        "id": "8go5GirLDVOm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_relu(model, neurons, drop_out):\n",
        "  w_init_relu = initializers.HeNormal(seed=42)\n",
        "  b_init = initializers.Zeros()\n",
        "  l1 = 1e-3\n",
        "  l2 = 1e-3\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(layers.Dense(neurons,\n",
        "                        activation='relu',\n",
        "                        kernel_initializer=w_init_relu,\n",
        "                        bias_initializer=b_init,\n",
        "                        kernel_regularizer=regularizers.L1L2(l1=l1, l2=l2),\n",
        "                        bias_regularizer=regularizers.L1L2(l1=l1, l2=l2),\n",
        "                        activity_regularizer=regularizers.L1L2(l1=l1, l2=l2)\n",
        "                         )\n",
        "  )\n",
        "  model.add(layers.Dropout(drop_out))\n",
        "  return model"
      ],
      "metadata": {
        "id": "BYi3VT0hc-W5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_model(run_dir, hparams):\n",
        "\n",
        "    # w_init_relu = initializers.HeNormal(seed=42)\n",
        "    # b_init = initializers.Zeros()\n",
        "    # l1 = 0.01\n",
        "    # l2 = 0.01\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "    model = layer_relu(model, hparams[HP_NUM_UNITS], hparams[HP_DROPOUT])\n",
        "    model = layer_relu(model, hparams[HP_NUM_UNITS], hparams[HP_DROPOUT])\n",
        "    model = layer_relu(model, hparams[HP_NUM_UNITS], hparams[HP_DROPOUT])\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # model = tf.keras.models.Sequential([\n",
        "    #     Flatten(input_shape=(28, 28)),\n",
        "    #     BatchNormalization(),\n",
        "    #     Dense(hparams[HP_NUM_UNITS], activation='relu', kernel_initializer=w_init_relu, bias_initializer=b_init,),\n",
        "    #     Dropout(hparams[HP_DROPOUT]),\n",
        "    #     BatchNormalization(),\n",
        "    #     Dense(hparams[HP_NUM_UNITS], activation='relu', kernel_initializer=w_init_relu, bias_initializer=b_init),\n",
        "    #     Dropout(hparams[HP_DROPOUT]),\n",
        "    #     BatchNormalization(),\n",
        "    #     Dense(hparams[HP_NUM_UNITS], activation='relu', kernel_initializer=w_init_relu, bias_initializer=b_init),\n",
        "    #     Dropout(hparams[HP_DROPOUT]),\n",
        "    #     Dense(10, activation='softmax')\n",
        "    # ])\n",
        "    model.compile(\n",
        "        optimizer=hparams[HP_OPTIMIZER],\n",
        "        loss='categorical_crossentropy', metrics=['accuracy'],\n",
        "    )\n",
        "\n",
        "    callback_list = [\n",
        "        EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=6),\n",
        "        ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True),\n",
        "        TensorBoard(log_dir=run_dir, histogram_freq=1)\n",
        "    ]\n",
        "    # Train the model\n",
        "    model.fit(x=x_train,\n",
        "              y=y_train,\n",
        "              epochs=100,\n",
        "              batch_size=128,\n",
        "              validation_split=0.13,\n",
        "              callbacks=callback_list,\n",
        "              verbose=0\n",
        "              )\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")\n",
        "    return test_accuracy\n"
      ],
      "metadata": {
        "id": "NqxYhvxFD8kh"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    accuracy = train_test_model(run_dir, hparams)\n",
        "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=2)"
      ],
      "metadata": {
        "id": "hePW-s4PEVde"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_num = 0\n",
        "\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
        "    for optimizer in HP_OPTIMIZER.domain.values:\n",
        "      hparams = {\n",
        "          HP_NUM_UNITS: num_units,\n",
        "          HP_DROPOUT: dropout_rate,\n",
        "          HP_OPTIMIZER: optimizer,\n",
        "      }\n",
        "      run_name = \"run-%d\" % session_num\n",
        "      run_name = run_name + f\"[{optimizer}] [{dropout_rate}] [{num_units}]\"\n",
        "      print('--- Starting trial: %s' % run_name)\n",
        "      print({h.name: hparams[h] for h in hparams})\n",
        "\n",
        "      run(log_dir + run_name, hparams)\n",
        "      session_num += 1"
      ],
      "metadata": {
        "id": "pvUvxZBqEXxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c586eae-fada-462f-c64c-a6c11aa18f79",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting trial: run-0[adam] [0.3] [512]\n",
            "{'num_units': 512, 'dropout': 0.3, 'optimizer': 'adam'}\n",
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.76936, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_accuracy improved from 0.76936 to 0.79064, saving model to best_model.h5\n",
            "\n",
            "Epoch 3: val_accuracy improved from 0.79064 to 0.79936, saving model to best_model.h5\n",
            "\n",
            "Epoch 4: val_accuracy improved from 0.79936 to 0.80782, saving model to best_model.h5\n",
            "\n",
            "Epoch 5: val_accuracy did not improve from 0.80782\n",
            "\n",
            "Epoch 6: val_accuracy did not improve from 0.80782\n",
            "\n",
            "Epoch 7: val_accuracy did not improve from 0.80782\n",
            "\n",
            "Epoch 8: val_accuracy improved from 0.80782 to 0.81179, saving model to best_model.h5\n",
            "\n",
            "Epoch 9: val_accuracy did not improve from 0.81179\n",
            "\n",
            "Epoch 10: val_accuracy improved from 0.81179 to 0.81385, saving model to best_model.h5\n",
            "\n",
            "Epoch 11: val_accuracy did not improve from 0.81385\n",
            "\n",
            "Epoch 12: val_accuracy did not improve from 0.81385\n",
            "\n",
            "Epoch 13: val_accuracy improved from 0.81385 to 0.81923, saving model to best_model.h5\n",
            "\n",
            "Epoch 14: val_accuracy improved from 0.81923 to 0.82128, saving model to best_model.h5\n",
            "\n",
            "Epoch 15: val_accuracy did not improve from 0.82128\n",
            "\n",
            "Epoch 16: val_accuracy improved from 0.82128 to 0.82756, saving model to best_model.h5\n",
            "\n",
            "Epoch 17: val_accuracy did not improve from 0.82756\n",
            "\n",
            "Epoch 18: val_accuracy did not improve from 0.82756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "# print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "ZMEhnbnl3Xax",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "Cw0tPvrV4VdG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/hparam_tuning"
      ],
      "metadata": {
        "id": "jled6KGdUzai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}